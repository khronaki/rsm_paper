
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \kc{Intro from big-little paper}
%\kc{write Intro}
Since the end of Dennard scaling~\cite{Dennard74} and the subsequent stagnation of CPU clock frequencies, computer architects and programmers rely on multi-core designs to achieve the desired performance levels.
While multi-core architectures constitute a solution to the CPU clock stagnation problem, they bring important challenges both from the hardware and software perspectives.
On the hardware side, multi-core architectures require sophisticated mechanisms in terms of coherence protocals, consistency models or deep memory hierarchies. 
Such requirements significantly complicate the hardware designing process.
On the software side, multi-core designs significantly complicate the programming burden with respect to their uni-core predecessors.
The different CPU's are exposed to the programmer, which has to make sure to use all of them efficiently, as well as using the memory hierarchy properly by exploiting both temporal and spatial locality.
This incresing programming complexity, which has been called the Programmability Wall~\cite{Chapman2007}, has motivated the advent of sophisticated programming paradigms and runtime system software to support them.

Task-based parallelism~\cite{Blumofe:PPoPP1995, Reinders2007, Bauer2012, OmpSs} has been proposed as a partial solution to the Programmability Wall and, indeed, the most relevant shared memory programming standards, like OpenMP, support tasking constructs~\cite{OpenMP4.0:Manual2013}.
The task based model requires the programmer to split the code into several sequential pieces, called tasks, as well as explicitely specifying their input and output dependencies. 
%It also requires these pieces of code to be annotated in terms of input or output data dependencies.
The task-based execution model has a master thread, which goes over the code of the application, that creates tasks once it finds source code annotations identifying them. 
A parallel runtime system manages the pool of all created tasks and schedules accross the several threads the parallel execution is composed of once their input dependencies are satisfied.
To carry out the task management process, the parallel runtime system creates and mantains a Task Dependency Graph (TDG) where nodes are tasks and edges are dependencies between them.
Once a new task is created, a new node is added to this TDG data structure. 
The connectivity of this new node is defined by the data dependencies of the task it represents, which are explictely specified at the application source code.
Oppositely, when the execution of a task finalizes, its corresponding node is removed from the TDG, as well as its data dependencies.

This task-based runtime system constitutes a software layer that enables parallel programmers to decouple the parallel code from the underlying parallel architecture where it is supposed to run on.
As long as the programmer is able to expose enough task parallelism to the parallel runtime system, the task-based execution model is able to properly manage it across homogeneous many-core architectures, heterogeneous designs with different core types. 
A common practice in the HPC context is to map a single thread per core, which enables the tasks running on that thread to fully use the core capacity. 
Also, mapping more than one thread per core and exploiting cores' Simlutaneous Multi-Threading (SMT) is also an option.
Finally, another important asset of task-based parallelism is the possibility of automatically managing executions on accelerators with different address spaces. 
Since the input and output dependencies of tasks are specified, the runtime system can automatically offload a task and its dependencies to an accelerator device (E.g. GPU) without the need for specific programmer intervention~\cite{Bueno:IPDPS2012}.
Additional optimizations in terms of software pre-fetching~\cite{Papaefstathiou2013} or more efficient coherence protocols~\cite{Manivannan2014} can also be enabled by the task-based paradigm.

Despite their advantages, task-based programming models also incur in some costs.
For example, the process of task creation requires the traversal of several indexed tables to update the status of parallel run by adding the new dependencies the recently created tasks bring, which brings a certain overhead.
Such overhead constitutes a significant burden, specially on architectures with several 10's or 100's of cores where tasks need to be created in a very fast rate to feed all of them.
This paper proposes the Task Generation eXpress ({\proposal}) approach, a combined hardware-software solution to eliminate the most important bottlenecks of task based parallelism without hurting their multiple advantages.
In particular, these are the contributions this paper makes beyond the state-of-the-art:

\begin{itemize}

\item A new kind of task-based runtime system that decouples the most costly routines from the other runtime activities and thus enables them be off-loaded to specific hardware based-accelerators.

\item A specific-purpose helper core able to accelerate the most time consuming runtime system activities. This hardware is also able to run user-level tasks, alhtough it does so in a very slow way if compared to a general purpose core.

\item A complete evaluation via trace-driven simulation considering 11 parallel OpenMP codes and $YYY$ different system configurations, including homogeneous and heterogeneous systems and large core counts up to $XXX$ cores. 
Our evaluation demonstrates how {\proposal} achieves average speedups of $ZZZZ$ when compared against currently use state-of-the-art approaches.

\end{itemize} 

The rest of this document is organized as follows: 
Section~\ref{sec:background} describes the task-based execution model and its main bottlenecks.
Section~\ref{sec:ram} the new task-based runtime system this paper proposes as well as the the specialized hardware that accelerates the most time-consuming runtime rutines.
Section~\ref{sec:experimental} contains the experimental setup of this paper.
Section~\ref{sec:evaluation} describes our evaluation of {\proposal} via trace-driven simulation.
Finally, Section~\ref{sec:related} discusses related work and Section~\ref{sec:conclusions} concludes 
this work.

